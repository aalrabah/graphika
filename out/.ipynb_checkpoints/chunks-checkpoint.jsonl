{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0000", "chunk_index": 0, "page_numbers": [1], "text": "I\nUNIVERSITY OF\nILLINOIS\nURBANA-CHAMPAIGN\nCONVAI\nCDS", "prev_chunk_id": null, "next_chunk_id": "ALecture3_DeepLearning_Background__0001"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0001", "chunk_index": 1, "page_numbers": [1], "text": "Dilek HakkaniT√ºr\nSieb\nCom and I", "prev_chunk_id": "ALecture3_DeepLearning_Background__0000", "next_chunk_id": "ALecture3_DeepLearning_Background__0002"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0002", "chunk_index": 2, "page_numbers": [2], "text": "- -Gradient Descent\n- -Softmax Regression\n- -Multi-layer Perceptron", "prev_chunk_id": "ALecture3_DeepLearning_Background__0001", "next_chunk_id": "ALecture3_DeepLearning_Background__0003"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0003", "chunk_index": 3, "page_numbers": [3], "text": "CONVAI\n- -Linear regression is a single-layer neural network, consisting of just a single neuron!\n- -Fully connected layer (also called dense layer): every input is connected to every output.", "prev_chunk_id": "ALecture3_DeepLearning_Background__0002", "next_chunk_id": "ALecture3_DeepLearning_Background__0004"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0004", "chunk_index": 4, "page_numbers": [4], "text": "- -Seen the solution to linear regression last Thursday!\n- -Even when we cannot solve the models analytically, we can still train models effectively by iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function:\n- -Derivative of the true loss (i.e., average of the losses computed over all examples in the training data).\n- -Expressing the updates mathematically:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0003", "next_chunk_id": "ALecture3_DeepLearning_Background__0005"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0005", "chunk_index": 5, "page_numbers": [5], "text": "CONVAI\n5\n4\n* 3\n2\n1\nO.D\nPredicted\nBest Fit", "prev_chunk_id": "ALecture3_DeepLearning_Background__0004", "next_chunk_id": "ALecture3_DeepLearning_Background__0006"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0006", "chunk_index": 6, "page_numbers": [6], "text": "- -Steps of the algorithm:\n1. initialize the values of the model parameters, typically at random\n2. iteratively update the parameters in the direction of the negative gradient.", "prev_chunk_id": "ALecture3_DeepLearning_Background__0005", "next_chunk_id": "ALecture3_DeepLearning_Background__0007"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0007", "chunk_index": 7, "page_numbers": [6, 7], "text": ": model parameters ( ùê∞ , ùëè\n```\nùõ≥ C ( ùõ≥ ): ùêø ( ùê∞ , ùëè )\n```\n)\nly start at 0", "prev_chunk_id": "ALecture3_DeepLearning_Background__0006", "next_chunk_id": "ALecture3_DeepLearning_Background__0008"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0008", "chunk_index": 8, "page_numbers": [7, 8, 9, 10], "text": "CONVAI\ne dC(@1) /do: 02+01-1\n- -Assume that Œ∏ has only one variable\nCONVAI\n- -Assume that Œ∏ has two variables { Œ∏ 1 , Œ∏ 2 }\n0.9\nupdate parameters:\nCONVAI\n002\n- -Assume that Œ∏ has two variables { Œ∏ 1 , Œ∏ 2 }\n(0) at 01: VoC(0') =\n- Randomly start at ùúÉ 0 :\n002\n- Compute the gradients of ùê∂ ùúÉ at ùúÉ 0 :\n- Update parameters:\n- Compute the gradients of ùê∂ ùúÉ at ùúÉ 1 : ¬∑ ‚Ä¶", "prev_chunk_id": "ALecture3_DeepLearning_Background__0007", "next_chunk_id": "ALecture3_DeepLearning_Background__0009"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0009", "chunk_index": 9, "page_numbers": [12], "text": "CONVAI\nVC (0) = K\n1\nAfter seeing all training samples, the model can be updated ‚Üí slow\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0008", "next_chunk_id": "ALecture3_DeepLearning_Background__0010"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0010", "chunk_index": 10, "page_numbers": [13], "text": "CONVAI\nc Gradient Descent (SGD)\n- -Gradient Descent\n- -Stochastic Gradient Descent (SGD)\n- -Pick a training sample x k\nTraining Data ( ) ( ) ÔÅª ÔÅΩ ÔÅã , ÀÜ , , ÀÜ , 2 2 1 1 y x y x\n- -If all training samples have the same probability to be picked, then\nThe model can be updated after seeing one training sample ‚Üí faster", "prev_chunk_id": "ALecture3_DeepLearning_Background__0009", "next_chunk_id": "ALecture3_DeepLearning_Background__0011"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0011", "chunk_index": 11, "page_numbers": [14], "text": "CONVAI\noK+1=0K-NVC1(0K)\n- -When running SGD, the model starts Œ∏ 0\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nsee all training samples once\n‚Üí one epoch", "prev_chunk_id": "ALecture3_DeepLearning_Background__0010", "next_chunk_id": "ALecture3_DeepLearning_Background__0012"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0012", "chunk_index": 12, "page_numbers": [15], "text": "CONVAI\n(SGD)\n- -Batch Gradient Descent\nUse all K samples in each iteration\n- -Stochastic Gradient Descent (SGD)\n- -Pick a training sample x k\nUse 1 sample in each iteration\n- -Mini-Batch SGD\n- -Pick a set of B training samples as a batch b\nUse all B samples in each iteration\nB is the 'batch size'", "prev_chunk_id": "ALecture3_DeepLearning_Background__0011", "next_chunk_id": "ALecture3_DeepLearning_Background__0013"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0013", "chunk_index": 13, "page_numbers": [16], "text": "- -Gradient Descent\n- -Softmax Regression\n- -Multi-layer Perceptron", "prev_chunk_id": "ALecture3_DeepLearning_Background__0012", "next_chunk_id": "ALecture3_DeepLearning_Background__0014"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0014", "chunk_index": 14, "page_numbers": [17], "text": "- -Regression is useful for 'how much?' and 'how many?' questions, where the target can take a real value.\n- -Many NLP problems are about 'which one?'\n- -Sentiment classification: positive, negative(, neutral)\n- -Sentence boundary detection from text: is the punctuation mark defining a sentence boundary or not\n- -What is the part-ofspeech tag of the word 'word' in this sentence? NOUN, VERB, etc.?\n- -We can represent target labels with one-hot encodings as well. For example, for sentiment classification:\nùë¶ ‚àà {(1,0,0),(0,1,0),(0,0,1)}.\nPositive   Negative  Neutral", "prev_chunk_id": "ALecture3_DeepLearning_Background__0013", "next_chunk_id": "ALecture3_DeepLearning_Background__0015"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0015", "chunk_index": 15, "page_numbers": [18], "text": "CONVAI\nInput layer\nX1\nX2\nX4\n- -To estimate the conditional probabilities associated with each class, we need a model with multiple outputs, one per class.\n- -As many linear functions as we have outputs.\n- -Example: Assume our input is represented with 4 features, then to compute the logits ùëú 1 , ùëú 2 , ùëú 3 , for each output, we need:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0014", "next_chunk_id": "ALecture3_DeepLearning_Background__0016"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0016", "chunk_index": 16, "page_numbers": [19], "text": "- -Need to interpret model outputs as probabilities and optimize our parameters to produce probabilities that maximize the likelihood of the observed data\n- -To generate predictions, we can set a threshold or choose the argmax of the predicted probabilities\n- -Interpret the logits ùëú directly as our outputs of interest, however:\n- -Nothing constrains these numbers to sum to 1.\n- -Depending on the inputs, they can take negative values.", "prev_chunk_id": "ALecture3_DeepLearning_Background__0015", "next_chunk_id": "ALecture3_DeepLearning_Background__0017"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0017", "chunk_index": 17, "page_numbers": [20], "text": "CONVAI\nwhere f; =\nexp(o;)\nL; exp(o;)\n- -Transforms logits such that they become nonnegative and sum to 1, while requiring that the model remains differentiable.\n- -ùë¶ 1+ ùë¶ 2+ ùë¶ 3 = 1,\nÃÇ\nÃÇ\nÃÇ\nÃÇ\n- -0‚â§ ùë¶ ùëñ ‚â§1 for all i , and\n- -The ordering of the logits has not changed; hence we can still pick the output using argmax!", "prev_chunk_id": "ALecture3_DeepLearning_Background__0016", "next_chunk_id": "ALecture3_DeepLearning_Background__0018"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0018", "chunk_index": 18, "page_numbers": [21], "text": "CONVAI\ni=1\n- -The softmax function gives us a vector ùê≤ ÃÇ, interpreted as estimated conditional probabilities of each class given the input ùë• , e.g., ùë¶ 1 = ùëÉ ( ùë¶ =cat ‚à£ ùê± ).\n- -Maximizing ùëÉ ( ùëå ‚à£ ùëã ) (and thus equivalently minimizing -log ùëÉ ( ùëå ‚à£ ùëã )) corresponds to predicting the label well!\nÃÇ", "prev_chunk_id": "ALecture3_DeepLearning_Background__0017", "next_chunk_id": "ALecture3_DeepLearning_Background__0019"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0019", "chunk_index": 19, "page_numbers": [22], "text": "CONVAI\ni=l\n.j=1\n=\ny; log j=1\nq : the number of classes exp(ok) - k=1\n= 10g > exp(ok) - 2\n- -If we add ùëú into the definition of the loss ùëô and use the definition of the softmax we obtain:\ndo, 1(y, √Ω) =\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -If we compute its derivative with respect to ùëú j , we get:\n<!-- formula-not-decoded -->\nThe gradient is the difference between the probability assigned to the true class by our model. This makes computing gradients very easy in practice!", "prev_chunk_id": "ALecture3_DeepLearning_Background__0018", "next_chunk_id": "ALecture3_DeepLearning_Background__0020"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0020", "chunk_index": 20, "page_numbers": [23], "text": "- -Gradient Descent\n- -Softmax Regression\n- -Multi-layer Perceptron", "prev_chunk_id": "ALecture3_DeepLearning_Background__0019", "next_chunk_id": "ALecture3_DeepLearning_Background__0021"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0021", "chunk_index": 21, "page_numbers": [24], "text": "CONVAI\nDegree = 1\nDegree = 2\n- -Linear regression inputs directly to our outputs via a single linear transformation.\n- -But what if our data is not linear?\nOutput layer\nInput layer\nX2\n.‚Ä¢‚Ä¢\n01)", "prev_chunk_id": "ALecture3_DeepLearning_Background__0020", "next_chunk_id": "ALecture3_DeepLearning_Background__0022"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0022", "chunk_index": 22, "page_numbers": [25, 26], "text": "CONVAI\n0.5\nZ\nSUC", "prev_chunk_id": "ALecture3_DeepLearning_Background__0021", "next_chunk_id": "ALecture3_DeepLearning_Background__0023"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0023", "chunk_index": 23, "page_numbers": [26], "text": "CONVAI\n- -Function approximation\n- -Without non-linearity , deep neural networks work the same as linear transform\n- -With non-linearity , networks with more layers can approximate more complex functions\nFigure from: http://cs224d.stanford.edu/lectures/CS224d-Lecture4.pdf", "prev_chunk_id": "ALecture3_DeepLearning_Background__0022", "next_chunk_id": "ALecture3_DeepLearning_Background__0024"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0024", "chunk_index": 24, "page_numbers": [27], "text": "CONVAI\n<!-- formula-not-decoded -->\nA single neuron can only handle binary classification", "prev_chunk_id": "ALecture3_DeepLearning_Background__0023", "next_chunk_id": "ALecture3_DeepLearning_Background__0025"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0025", "chunk_index": 25, "page_numbers": [28], "text": "CONVAI\n- -Handwriting digit classification\nA layer of neurons can handle multiple possible output, and the result depends on the max one\n<!-- formula-not-decoded -->\nWhich one is max?", "prev_chunk_id": "ALecture3_DeepLearning_Background__0024", "next_chunk_id": "ALecture3_DeepLearning_Background__0026"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0026", "chunk_index": 26, "page_numbers": [29], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0025", "next_chunk_id": "ALecture3_DeepLearning_Background__0027"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0027", "chunk_index": 27, "page_numbers": [30], "text": "CONVAI", "prev_chunk_id": "ALecture3_DeepLearning_Background__0026", "next_chunk_id": "ALecture3_DeepLearning_Background__0028"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0028", "chunk_index": 28, "page_numbers": [31, 32, 33], "text": "CONVAI\nW'=\nW21\nW22\n‚Ä¢\n‚Ä¢\nweights between two layers ‚Üí a matrix\nNy\nCONVAI\nbias of all neurons at each layer ‚Üí a vector\nCONVAI\ni\nb; : :\nLayer I\nN, nodes\n1-1 + 61", "prev_chunk_id": "ALecture3_DeepLearning_Background__0027", "next_chunk_id": "ALecture3_DeepLearning_Background__0029"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0029", "chunk_index": 29, "page_numbers": [34], "text": "l\ni a l a l i z l i b l b : output of a neuron : output vector of a layer : input of activation function activation function for a : a weight : a weight matrix : a bias : a bias vector\nl z : input vector of layer", "prev_chunk_id": "ALecture3_DeepLearning_Background__0028", "next_chunk_id": "ALecture3_DeepLearning_Background__0030"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0030", "chunk_index": 30, "page_numbers": [35], "text": "l", "prev_chunk_id": "ALecture3_DeepLearning_Background__0029", "next_chunk_id": "ALecture3_DeepLearning_Background__0031"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0031", "chunk_index": 31, "page_numbers": [36], "text": "CONVAI\nW21\n61\nWlal+ wizan1+...+61\n:\n. .", "prev_chunk_id": "ALecture3_DeepLearning_Background__0030", "next_chunk_id": "ALecture3_DeepLearning_Background__0032"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0032", "chunk_index": 32, "page_numbers": [37], "text": "l\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0031", "next_chunk_id": "ALecture3_DeepLearning_Background__0033"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0033", "chunk_index": 33, "page_numbers": [38, 39], "text": "CONVAI\nLay\nLa\n‚Üí –£–º", "prev_chunk_id": "ALecture3_DeepLearning_Background__0032", "next_chunk_id": "ALecture3_DeepLearning_Background__0034"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0034", "chunk_index": 34, "page_numbers": [39, 40], "text": "CONVAI\no (W'x+64 01W7a'+63) 0WTa-1+64) =Y\n- -Fully connected feedforward network\n<!-- formula-not-decoded -->\nLay\nLa\nN\nCONVAI\n- -Fully connected feedforward network\n<!-- formula-not-decoded -->\n\"2\"\na\nL\na\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0033", "next_chunk_id": "ALecture3_DeepLearning_Background__0035"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0035", "chunk_index": 35, "page_numbers": [41, 42], "text": "CONVAI\ne loss\nBack to an earlier slide!\nupdate parameters", "prev_chunk_id": "ALecture3_DeepLearning_Background__0034", "next_chunk_id": "ALecture3_DeepLearning_Background__0036"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0036", "chunk_index": 36, "page_numbers": [42], "text": "CONVAI\nW'=\n61\nW21 W22\naC(0)\nVC(0) =\nduij aC(0)\nab!\n,62, ..Wh\n<!-- formula-not-decoded -->\nAlgorithm\n```\nAlgorithm Initialization: start at ùúÉ 0 while( ùúÉ (ùëñ+1) ‚â† ùúÉ ùëñ ) { compute gradient at ùúÉ ùëñ update parameters }\n```\nTo update weights efficiently, we use backpropagation .", "prev_chunk_id": "ALecture3_DeepLearning_Background__0035", "next_chunk_id": "ALecture3_DeepLearning_Background__0037"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0037", "chunk_index": 37, "page_numbers": [43], "text": "- -forward propagation\n- -from input ùë• to output ùë¶ information flows forward through the network\n- -during training, forward propagation can continue onward until it produces a scalar loss C ( Œ∏ )\n- -back-propagation\n- -allows the information from the cost to then flow backwards through the network, in order to compute the gradient\n- -can be applied to any function", "prev_chunk_id": "ALecture3_DeepLearning_Background__0036", "next_chunk_id": "ALecture3_DeepLearning_Background__0038"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0038", "chunk_index": 38, "page_numbers": [44, 45], "text": "CONVAI\n= f (y) f'(x) f'(w)\n<!-- formula-not-decoded -->\nforward propagation for computing loss\n<!-- formula-not-decoded -->\nback-propagation of gradients for updating weights\nupdate parameters", "prev_chunk_id": "ALecture3_DeepLearning_Background__0037", "next_chunk_id": "ALecture3_DeepLearning_Background__0039"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0039", "chunk_index": 39, "page_numbers": [45], "text": "CONVAI\nWI=\n, 61\n–≥–æ , 20/2\nW21 W22\naC(0)\nVC(0) =\n,65}\nAlgorithm\nInitialization: start at 0¬∞\nwhile(0(i+1) # 0i)\n<!-- formula-not-decoded -->\nupdate parameters\n<!-- formula-not-decoded -->\n```\nAlgorithm Initialization: start at ùúÉ 0 while( ùúÉ (ùëñ+1) ‚â† ùúÉ ùëñ ) { compute gradient at ùúÉ ùëñ update parameters }\n```\nTo update weights efficiently, we use backpropagation .\ndC(0)", "prev_chunk_id": "ALecture3_DeepLearning_Background__0038", "next_chunk_id": "ALecture3_DeepLearning_Background__0040"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0040", "chunk_index": 40, "page_numbers": [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "text": "CONVAI\nazi\n<!-- formula-not-decoded -->\nCONVAI\ndz!\nƒëuij\n-= a;\n1-1\nCONVAI\nCONVAI\naz!\nawisi dz; –¥–∏—ñ;\n<!-- formula-not-decoded -->\nCONVAI\n–¥—à—ñ, dz; –¥–∏—ñ,\nCONVAI\nIdea: computing ùõø ùëô layer by layer (from ùõø ùêø to ùõø 1 ) is more efficient\nCONVAI\n- -Idea: from L to 1\n- ÔÇÅ Initialization: compute ùõø ùêø\n- ÔÇÇ Compute ùõø ùëô based on ùõø ùëô+1\nCONVAI\n=\n–¥—É—ñ\n- -Idea: from L to 1\n- ÔÇÅ Initialization: compute ùúπ ùë≥\n- ÔÇÇ Compute ùõø ùëô based on ùõø ùëô+1\nùúïùê∂Œ§ùúïùë¶ùëñ depends on the loss function\nCONVAI\nalization: compute 8\noute s' based on 8l+1\n$ =.\n=\n<!-- formula-not-decoded -->\n- -Idea: from L to 1\n6'(24) |\n- ÔÇÅ Initialization: compute ùúπ ùë≥\n- ÔÇÇ Compute ùõø ùëô based on ùõø ùëô+1\n–¥–°\n–¥—É—ñ\n‚Ä¢ Z\nIn\nVC (y) =\n<!-- formula-not-decoded -->\n0.5\nCONVAI\ndai azi\n<!-- formula-not-decoded -->\n- -Idea: from L to 1\n- ÔÇÅ Initialization: compute ùõø ùêø\n- ÔÇÇ Compute ùúπ ùíç based on ùúπ ùíç+ùüè 1 + l\n1\nŒîz\nŒîz\n1\n+\nl\n2\nŒîz\nl\nk\n‚Ä¶ ‚Ä¶\n+\n1\nŒîz\nl\ni\n‚Üí\nŒîa\nl\ni\nWij\nCONVAI\nCONVAI\nink the propagation\n- -Rethink the propagation\nCONVAI\nd' (2) =\no'(24)|\nd' (22)\no' (2!)\n<!-- formula-not-decoded -->\n- -Idea: from L to 1\nCONVAI\n<!-- formula-not-decoded -->\n- ÔÇÅ Initialization: compute ùõø ùêø\n- ÔÇÇ Compute ùõø ùëô-1 based on ùõø ùëô", "prev_chunk_id": "ALecture3_DeepLearning_Background__0039", "next_chunk_id": "ALecture3_DeepLearning_Background__0041"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0041", "chunk_index": 41, "page_numbers": [60], "text": "CONVAI\naw!;\nxj, b=1\nForward Pass\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0040", "next_chunk_id": "ALecture3_DeepLearning_Background__0042"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0042", "chunk_index": 42, "page_numbers": [60], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0041", "next_chunk_id": "ALecture3_DeepLearning_Background__0043"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0043", "chunk_index": 43, "page_numbers": [61], "text": "CONVAI\n= √≥' (2) ‚Ä¢ (W'+1)I g'+1", "prev_chunk_id": "ALecture3_DeepLearning_Background__0042", "next_chunk_id": "ALecture3_DeepLearning_Background__0044"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0044", "chunk_index": 44, "page_numbers": [61], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0043", "next_chunk_id": "ALecture3_DeepLearning_Background__0045"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0045", "chunk_index": 45, "page_numbers": [62], "text": "CONVAI", "prev_chunk_id": "ALecture3_DeepLearning_Background__0044", "next_chunk_id": "ALecture3_DeepLearning_Background__0046"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0046", "chunk_index": 46, "page_numbers": [63], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture3_DeepLearning_Background__0045", "next_chunk_id": null}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0000", "chunk_index": 0, "page_numbers": [1], "text": "I\nUNIVERSITY OF\nILLINOIS\nURBANA-CHAMPAIGN\nCONVAI\nCDS", "prev_chunk_id": null, "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0001"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0001", "chunk_index": 1, "page_numbers": [1], "text": "Dilek HakkaniT√ºr\nSieb\nCom and I", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0000", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0002"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0002", "chunk_index": 2, "page_numbers": [2], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0001", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0003"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0003", "chunk_index": 3, "page_numbers": [3], "text": "- Typically, words are treated as discrete, arbitrary symbols in NLP systems.\n- But words have lots of interesting relationships to each other!\n- A lot of the previous work examined how to represent these:\n- Manually-built resources like WordNet provide one way to define words and their similarities\n- Distributional representations  and word embeddings are another way\n- And they can be  learned automatically from large text collections", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0002", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0004"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0004", "chunk_index": 4, "page_numbers": [4], "text": "- -Characterizes knowledge of a word in terms of 'the company it keeps' (Firth, 1957)\n- -Word categories can be defined by the context in which they appear\n- -Such characterization is the idea behind static word representations estimated from data (Mikolov et al., 2013) and modern large language models.\n- -For a word w, find all the contexts w 1 ww2 in which w appears\n- -Find all words w' that share many frequent contexts\n- -Use a LARGE text collection to get good counts", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0003", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0005"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0005", "chunk_index": 5, "page_numbers": [5], "text": "- -Representing discrete types (i.e., words) as vectors\n- -Each word = a vector\n- -Similar words are \"nearby in space\"", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0004", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0006"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0006", "chunk_index": 6, "page_numbers": [6], "text": "- -Called an \"embedding\" because it's embedded into a space\n- -The standard way to represent meaning in NLP\n- -Fine-grained model of meaning for similarity\n- -NLP tasks like sentiment analysis\n- -With words,  requires same word to be in training and test\n- -With embeddings: ok if similar words occur.\n- -Question answering, conversational agents, etc.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0005", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0007"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0007", "chunk_index": 7, "page_numbers": [7], "text": "- -Term frequency (TF), term frequency, inverse document frequency (TF.IDF)\n- -A common baseline model\n- -Words are represented by a simple function of the counts of nearby words", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0006", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0008"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0008", "chunk_index": 8, "page_numbers": [7], "text": "- -E.g. Word2Vec, Representation is created by training a classifier to distinguish nearby and far-away words", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0007", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0009"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0009", "chunk_index": 9, "page_numbers": [8], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0008", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0010"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0010", "chunk_index": 10, "page_numbers": [9, 10], "text": "apricot, aardvark = 0. apricot, computer digital = 0. apricot, data = 0. apricot, pinch = 1. apricot, result = 0. apricot, sugar = 1. apricot, ‚Ä¶ = . pineapple, aardvark = 0. pineapple, computer digital = 0. pineapple, data = 0. pineapple, pinch = 1. pineapple, result = 0. pineapple, sugar = 1. pineapple, ‚Ä¶ = . digital computer, aardvark = 0. digital computer, computer digital = 2. digital computer, data = 1. digital computer, pinch = 0. digital computer, result = 1. digital computer, sugar = 0. digital computer, ‚Ä¶ = . information, aardvark = 0. information, computer digital = 1. information, data = 6. information, pinch = 0. information, result = 4. information, sugar = 0. information, ‚Ä¶ = \n9\nall\n1", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0009", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0011"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0011", "chunk_index": 11, "page_numbers": [10, 11], "text": "CONVAI\n- -Count how often 'apple' occurs close to other words in a large text collection (corpus):\n- -Do the same for 'orange':\n- -Interpret counts as coordinates.\neat, 1 = fall. eat, 2 = ripe. eat, 3 = slice. eat, 4 = peel. eat, 5 = tree. eat, 6 = throw. eat, 7 = fruit. eat, 8 = pie. eat, 9 = bite. eat, 10 = crab. 794, 1 = 244. 794, 2 = 47. 794, 3 = 221. 794, 4 = 208. 794, 5 = 160. 794, 6 = 145. 794, 7 = 156. 794, 8 = 109. 794, 9 = 104. 794, 10 = 88\neat, 1 = fall. eat, 2 = ripe. eat, 3 = slice. eat, 4 = peel. eat, 5 = tree. eat, 6 = throw. eat, 7 = fruit. eat, 8 = pie. eat, 9 = bite. eat, 10 = crab. 265, 1 = 22. 265, 2 = 25. 265, 3 = 62. 265, 4 = 220. 265, 5 = 64. 265, 6 = 74. 265, 7 = 111. 265, 8 = 4. 265, 9 = 4. 265, 10 = 8\nEvery context word becomes a dimension.\n1", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0010", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0012"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0012", "chunk_index": 12, "page_numbers": [11], "text": "CONVAI\norange\nH\nea\nSimilarity between two words as proximity in space", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0011", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0013"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0013", "chunk_index": 13, "page_numbers": [12], "text": "Use the angle between vectors instead of point distance to get around word frequency issues", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0012", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0014"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0014", "chunk_index": 14, "page_numbers": [13], "text": "- -Function words co-occur frequently with all words\n- -That makes them less informative\n- -They have much higher co-occurrence counts than content words\n- -They can 'drown out' more informative contexts\n- -Some counts for 'letter' in 'Pride and Prejudice'.\nthe, 1 = to. the, 2 = of. the, 3 = and. the, 4 = a. the, 5 = her. the, 6 = she. the, 7 = his. the, 8 = is. the, 9 = was. the, 10 = in. the, 11 = that. 102, 1 = 75. 102, 2 = 72. 102, 3 = 56. 102, 4 = 52. 102, 5 = 50. 102, 6 = 41. 102, 7 = 36. 102, 8 = 35. 102, 9 = 34. 102, 10 = 34. 102, 11 = 33. had, 1 = i from. had, 2 = i from. had, 3 = you. had, 4 = as. had, 5 = . had, 6 = this. had, 7 = mr. had, 8 = for. had, 9 = on. had, 10 = be. had, 11 = he. 32, 1 = 28 28 25. 32, 2 = 28 28 25. 32, 3 = 28 28 25. 32, 4 = 23 23 with. 32, 5 = 23 23 with. 32, 6 = 22 him. 32, 7 = 22 him. 32, 8 = 21 which. 32, 9 = 20 by when. 32, 10 = 18. 32, 11 = 17 jane. but 17, 1 = but 17. but 17, 2 = elizabeth 17. but 17, 3 = elizabeth 17. but 17, 4 = 16. but 17, 5 = 16. but 17, 6 = 16. but 17, 7 = 16. but 17, 8 = 16. but 17, 9 = 15 14. but 17, 10 = 15 14. but 17, 11 = 12\nAll the most frequent cooccurring words are function words.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0013", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0015"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0015", "chunk_index": 15, "page_numbers": [14], "text": "- tf: term frequency . frequency count (usually log-transformed):\n- Idf: inverse document frequency:\nWords like \"the\" or \"good\" have very low idf\ntf-idf value for word t in document d:\nAlternative to tf.idf: pointwise mutual information", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0014", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0016"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0016", "chunk_index": 16, "page_numbers": [15], "text": "- -Whether a context word is particularly informative about the target word.\n- -Degree of association between target and context,  rather than cooccurrence:\n- -High association: high co-occurrence with specific words, lower with everything else\n- -Low association: lots of co-occurrence with all words", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0015", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0017"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0017", "chunk_index": 17, "page_numbers": [16], "text": "Do events x and y co-occur more than if they were independent?\n<!-- formula-not-decoded -->\nPMI between two words :  (Church & Hanks 1989)\nDo words x and y co-occur more than if they were independent?\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0016", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0018"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0018", "chunk_index": 18, "page_numbers": [17], "text": "- -long (length |V|= 20,000 to 50,000)\n- -sparse (most elements are zero)", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0017", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0019"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0019", "chunk_index": 19, "page_numbers": [18], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0018", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0020"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0020", "chunk_index": 20, "page_numbers": [19], "text": "- -AIM: learn vectors to represent the semantics of a word from its contextual uses (just like distributional measure)\n- -Dense vectors\n- -Words that have similar meanings -> close to each other in the vector space different meanings -> far from each other\n- -short (length 50-1000)\n- -dense (most elements are non-zero)\n- -Why dense vectors?\n- -What are they good for?\n- -How are they built?\n- -How are they used?\n- -Why are they an improvement over distributional semantics precursor?", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0019", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0021"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0021", "chunk_index": 21, "page_numbers": [20], "text": "- -Why dense vectors?\n- -Short vectors may be easier to use as features in machine learning (less weights to tune)\n- -Dense vectors may generalize better than storing explicit counts\n- -They may do better at capturing synonymy:\n- -car and automobile are synonyms; but are distinct dimensions\n- -a word with car as a neighbor and a word with automobile as a neighbor should be similar, but aren't\n- -In practice, they work better", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0020", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0022"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0022", "chunk_index": 22, "page_numbers": [21], "text": "- -Train with just words (i.e., unlabeled data), in a supervised fashion, by constructing auxiliary tasks:\n- -Language modeling: Given a sequence of words, predict the next word:\n<!-- formula-not-decoded -->\n- -Given lexical context (i.e., previous and following words), predict the missing one ( CBOW model ):\nGiven w1 ,‚Ä¶,w n-1 , and w n+1 ,‚Ä¶, wn+k , predict w n\n- -Given a word, predict words that occur within a window (independent of their position) ( skip-gram model ):\nGiven wn , predict w 1 ,‚Ä¶,w n-1 , and w n+1 ,‚Ä¶, wn+k", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0021", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0023"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0023", "chunk_index": 23, "page_numbers": [22], "text": "CONVAI\n- -Word2Vec\n0.413\n- -Stanford's GLoVe\n- -Facebook's FastText\n- -Typical format of word embeddings:\n-0.212\n1.380\n...", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0022", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0024"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0024", "chunk_index": 24, "page_numbers": [23], "text": "- -Idea: predict rather than count\n- -Word2vec (Mikolov et al., 2013)\n- -https://code.google.com/archive/p/word2vec/\n- -Instead of counting how often each word w occurs near \"apricot\"\n- -Train a classifier on a binary prediction task:\n- -Is w likely to show up near \"apricot\"?\n- -Fake task: We don't actually care about this task\n- -But we'll take the learned classifier weights as the word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0023", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0025"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0025", "chunk_index": 25, "page_numbers": [24], "text": "CONVAI\nP(W0,1, W0,2,\" ., W0,0 | WT) = 1IP(W0,c |wT)\n- -Goal: predict surrounding words within a window of each word\n- -Objective function: maximize the probability of any context word given the current center word\n<!-- formula-not-decoded -->\n, –®–¢-1, –®–¢", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0024", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0026"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0026", "chunk_index": 26, "page_numbers": [25, 26], "text": "CONVAI\nProbability that the word\n\"abandon\" appears nearby\nInput Vector\n1\n0\n0\n10,000\npositions\nLinear Neurons\nN=\nW'\nN√óV\n- -Goal: predict surrounding words within a window of each word\nA '1' in the position corresponding to the\nword \"ants\"\nV=\n10,000 words", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0025", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0027"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0027", "chunk_index": 27, "page_numbers": [26], "text": "Hidden Layer\nWeight Matrix\nWord Vector\nLookup Table!", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0026", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0028"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0028", "chunk_index": 28, "page_numbers": [27, 28], "text": "Input Vector\n- -Hidden layer weight matrix = word vector lookup\nh = x\n[ 00\n1\n<!-- formula-not-decoded -->\n300 features\n<!-- formula-not-decoded -->\nCONVAI\nHidden Layer\nLinear Neurons\nEach vocabulary entry has two vectors: as a target word and as a context word\n<!-- formula-not-decoded -->\nCONVAI\nEach vocabulary entry has two vectors: as a target word and as a context word", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0027", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0029"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0029", "chunk_index": 29, "page_numbers": [29], "text": "CONVAI\nInput Vector\n‚Ä¢\nWV√óN\n300 features\nV\n10,000\npostiong\n=WIl\nLinear Neurons\nWIxv\nN\n=\nProbability that the word\n\"abandon\" appears nearby\nProbability that the word\n\"ability\" appears nearby\nA '1' in the position word \"ants\"\ncorresponding to the\n1\n0", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0028", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0030"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0030", "chunk_index": 30, "page_numbers": [30], "text": "- Training sentence:\n- ... lemon, a tablespoon of apricot preserves   or pinch ...\n- c1        c2      t c3             c4\n- For each positive example, create k negative examples.\n- Using noise words\n- Any random word that isn't t", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0029", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0031"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0031", "chunk_index": 31, "page_numbers": [31], "text": "- Training sentence:\n- ... lemon, a tablespoon of apricot preserves or pinch ...\n- c1         c2     t c3        c4", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0030", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0032"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0032", "chunk_index": 32, "page_numbers": [32], "text": "- -Could pick w according to their unigram frequency P(w)\n- -More common to chose them according to p Œ± (w)\n- -Œ± = ¬æ works well because it gives rare noise words slightly higher probability\n- -To show this, imagine two events p(a)=.99 and p(b) = .01:\nIdea: less frequent words sampled more often.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0031", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0033"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0033", "chunk_index": 33, "page_numbers": [33, 34], "text": "CONVAI\nWOMAN\n‚Ä¢ KING is similar to KINGS as MAN is similar to MEN\n- -Relations between words, captured by vector operations\n- -Lots of online lectures/tutorial\n- -https://www.slideshare.net/mlprague/tom-mikolov-distributedrepresentations-for-nlp\nCONVAI\nMAN\nUNCLE\nKINGS", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0032", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0034"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0034", "chunk_index": 34, "page_numbers": [34], "text": "vector( 'king' ) - vector( 'man' ) + vector( 'woman' ) ‚âà vector('queen') vector( 'Paris' ) - vector( 'France' ) + vector( 'Italy' ) ‚âà vector('Rome')\nFrom Presentation by Mikolov", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0033", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0035"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0035", "chunk_index": 35, "page_numbers": [35, 36, 37], "text": "CONVAI\n0.4 -\n0.3 -\n0.2 -\n0.1 -\n0 -\n-0.1 -\n-0.2 -\n-0.3 -\n-0.4-\n-0.5 -\n‚Ä¢ niece\n‚Ä¢ aunt y sister\n'' nephew\n\" brother\n-0.5\n-0.4\n‚Ä¢ countess\n/\n; duchess-\n0.5\n0.4 -\nCONVAI\n0.3 - slow-\nshort strong\nclear soft\ndark r\n-0.3\n0.2 -\n0.1 -\n0 -\n-0.1 -\n-0.2 -\n-0.3\n-0.4\nslower shorter\nstronger softer\n//\ndarker\n-0.1\n-0.2\n- ‚Ä¢ shortest\nCONVAI\nParis - France + Italy\nRome", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0034", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0036"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0036", "chunk_index": 36, "page_numbers": [37], "text": "sushi - Japan + Germany\nWindows - Microsoft + Google\nMontreal Canadiens - Montreal + Toronto bratwurst", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0035", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0037"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0037", "chunk_index": 37, "page_numbers": [38, 39], "text": "CONVAI\nCzech + currency\nVietnam + capital\nGerman + airlines\nRussian + river\nFrench + actress koruna, Czech crown, Polish zloty, CTK\nHanoi, Ho Chi Minh City, Viet Nam, Vietnamese\n0", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0036", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0038"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0038", "chunk_index": 38, "page_numbers": [39], "text": "-0.05 -\nCONVAI\n-0.1 -\n-0.15 -\n-0.2 -\n-0.25 -\n-0.3 -\n-0.35.8\ndrawn draw\ngiven\ngive", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0037", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0039"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0039", "chunk_index": 39, "page_numbers": [40], "text": "- -large amounts of monolingual data for source and target language\n- -Small amount of bilingual data\n- -Word Vectors have similar structures in two languages\n- -Need to learn projection from one language to another\n- -Can then translate any word seen in the monolingual data", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0038", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0040"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0040", "chunk_index": 40, "page_numbers": [41], "text": "CONVAI\n0.15 -\n0.1 -\n0.05\n0-\n-0.05 -\n-0.1 -\n-0.15 -\n-0.2 -\n-0.25 -|\n-0-8.3\n0.4 -\n0.3 -\n0.2 -\n0.1\n0-\nO caballo (horse)\no vaca (cow)\no cerdo (pig)\no cat\n-0.25\n-0.2\n-0.15 -0.1\nSimilar idea works for other NLP tasks and transferring models for NLP tasks too. For example, spoken language understanding (Upadhyay et al, IEEE ICASSP, 2018).\nCOW\n‚Ä¢ pig\n‚Ä¢ dog perro (dog)", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0039", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0041"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0041", "chunk_index": 41, "page_numbers": [42], "text": "- -Compare to human scores on word similarity-type tasks:\n- WordSim-353 (Finkelstein et al., 2002)\n- -SimLex-999 (Hill et al., 2015)\n- Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012)\n- TOEFL dataset: Levied is closest in meaning to: imposed, believed, requested, correlated", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0040", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0042"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0042", "chunk_index": 42, "page_numbers": [43], "text": "- -Goal: use word vectors in neural net models built for subsequent tasks\n- -Benefit\n- -Ability to also classify words accurately\n- -Ex. countries cluster together a classifying location words should be possible with word vectors\n- -Incorporate any information into them other tasks\n- -Ex. project sentiment into words to find most positive/negative words in corpus", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0041", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0043"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0043", "chunk_index": 43, "page_numbers": [44], "text": "- -Global Vectors for Word Representation (GloVe)\n- -Methods like skipgram do good on the analogy task, but‚Ä¶\n- -They poorly utilize the statistics of the corpus, since they train on separate local context windows instead of on global co-occurrence counts.\n- -A specific weighted least squares model that trains on global word-word cooccurrence counts and thus makes efficient use of statistics.\n- -GloVe learns based on a co-occurrence matrix and trains word vectors so their differences predict co-occurrence ratios.\n- -Source code for the model as well as trained word vectors at https://nlp.stanford.edu/projects/glove/\n- -Pennington et al., 2014\n- Newer Version: Carlson et al., 2025", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0042", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0044"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0044", "chunk_index": 44, "page_numbers": [45, 46], "text": "CONVAI\n- -Idea: ratio of co-occurrence probability can encode meaning\n- -Co-occurrence Probability, Pij is the probability that word wj appears in\n- the context of word wi\n- -Relationship between the words wi and wj\nP ( x | ice) P ( x | ice), x = solid x = solid = 1.9√ó10 -4 large. P ( x | ice) P ( x | ice), x = gas x = gas = 6.6√ó10 -5 small. P ( x | ice) P ( x | ice), x = water x = water = 3.0√ó10 -3 large. P ( x | ice) P ( x | ice), x = fashion x = random = 1.7√ó10 -5 small. P ( x | steam) P ( x | steam), x = solid x = solid = 2.2√ó10 -5 small. P ( x | steam) P ( x | steam), x = gas x = gas = 7.8√ó10 -4 large. P ( x | steam) P ( x | steam), x = water x = water = 2.2√ó10 -3 large. P ( x | steam) P ( x | steam), x = fashion x = random = 1.8√ó10 -5 small. P x | ice P x | steam P x | ice P x | steam, x = solid x = solid = 8.9 large. P x | ice P x | steam P x | ice P x | steam, x = gas x = gas = 8.5√ó10 -2 small. P x | ice P x | steam P x | ice P x | steam, x = water x = water = 1.36 ~ 1. P x | ice P x | steam P x | ice P x | steam, x = fashion x = random = 0.96 ~ 1\nThe relationship of wi and wj can be examined by studying the ratio of their co-occurrence probabilities with various probe words viouertmrule\nCONVAI\nPig = Xij/ Xi\nXmax i,j =1\n- -The model enforces this property by learning embeddings ùë§ùëñ, ‡∑• ùë§ùëó such that: 0.6 f(Xij) 0.4\na very frequent co-occurrences (so common\n- -New, weighted least squares regression:\nWeighting function\n- -f(x) is a weighting function that down-weights very rare and very frequent co-occurrences (so common words like ' the ' don ' t dominate).\n<!-- formula-not-decoded -->\nXij", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0043", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0045"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0045", "chunk_index": 45, "page_numbers": [47], "text": "- -Word2Vec uses local context (a sliding window over a corpus), whereas GloVe uses global co-occurrence statistics.\n- -GloVe needs big co-occurrence matrix upfront", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0044", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0046"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0046", "chunk_index": 46, "page_numbers": [48], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0045", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0047"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0047", "chunk_index": 47, "page_numbers": [49, 50], "text": "CONVAI\n- Goal: estimate the probability of a word sequence\n- Example task: determine whether a sequence is grammatical or makes more sense\nIf P(recognize speech)\n- > P(wreck a nice beach)\nOutput =  'recognize speech'\nrecognize speech or wreck a nice beach\nwulus probabilly uld\nm", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0046", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0048"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0048", "chunk_index": 48, "page_numbers": [50], "text": "CONVAI\nmodel\n- Goal: estimate the probability of a word sequence\n- N-gram language model\n- Probability is conditioned on a window of ( n1) previous words\n- Estimate the probability based on the training data\n<!-- formula-not-decoded -->\nIssue: some sequences may not appear in the training data\n, Wi-1)", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0047", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0049"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0049", "chunk_index": 49, "page_numbers": [51], "text": "CONVAI\n‚Üí smoothing\n- Training data:\n- The dog ran ‚Ä¶‚Ä¶\n- The cat jumped ‚Ä¶‚Ä¶\n- The probability is not accurate.\n- The phenomenon happens because we cannot collect all the possible text in the world as training data.\n```\nP( jumped | dog ) = 0 P( ran | cat ) = 0 0.0001 0.0001\n```\ngive some small probability ‚Üí smoothing", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0048", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0050"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0050", "chunk_index": 50, "page_numbers": [52], "text": "<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0049", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0051"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0051", "chunk_index": 51, "page_numbers": [53], "text": "- -Corpus of 1K tokens, |V| = 20, C(the) = 100 C(orange) = 14, C(apple) = 0\n- -Before smoothing:\n<!-- formula-not-decoded -->\nP(orange) = 14/1000 = 0.014\nP(apple) = 0/1000 = 0\n- -After smoothing:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0050", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0052"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0052", "chunk_index": 52, "page_numbers": [54], "text": "CONVAI\nprobability of the item\n0.25 -\n0.20 -\n0.15\n0.10-\n0.05 -\n0.00\n* Items (samples) are sorted in order of decreasing probability", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0051", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0053"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0053", "chunk_index": 53, "page_numbers": [55], "text": "CONVAI\n- -Idea: estimate not from counts, but based on the NN prediction\nP( ' wreck a nice beach ' ) = P(wreck|START)P(a|wreck)P(nice|a)P(beach|nice)", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0052", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0054"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0054", "chunk_index": 54, "page_numbers": [56, 57], "text": "CONVAI\nhidden\n000\nC(W,-n+\nTable look-up\nin C\nindex for W,-n+1\ni-th output = P(w, = i | context)\nsoftmax\nProbability distribution of\n<!-- formula-not-decoded -->\nW(3)\nW(2)\n- -The input layer (or hidden layer) of the related words are close\n- -If P(jump|dog) is large, P(jump|cat) increases accordingly (even there is no '‚Ä¶ cat jump ‚Ä¶' in the data)\nSmoothing is automatically done\nIssue: fixed context window for conditioning", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0053", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0055"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0055", "chunk_index": 55, "page_numbers": [58], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0054", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0056"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0056", "chunk_index": 56, "page_numbers": [59], "text": "CONVAI\n- 1\n- 2\nStrength of association of\n20\n40\nPercentage of workers in occupation who are women\nSemantics derived automatically from language corpora contain human-like biases, Aylin Caliskan, Joanna J. Bryson, Arvind Narayanan, Science 2017.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0055", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0057"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0057", "chunk_index": 57, "page_numbers": [60], "text": "CONVAI\ny =sis he\nx =brother\nAutomatically generate he : x :: she : y analogies.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0056", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0058"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0058", "chunk_index": 58, "page_numbers": [61], "text": "CONVAI\npizza programn\nhe\nSis\nAutomatically generate he : x :: she : y analogies.\nbrother", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0057", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0059"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0059", "chunk_index": 59, "page_numbers": [62], "text": "CONVAI\nMAN\nUNCLE\nKING\n- ùë£ùëöùëéùëõ - ùë£ùë§ùëúùëöùëéùëõ +ùë£ùë¢ùëõùëêùëôùëí ‚àº ùë£ùëéùë¢ùëõùë°\nhe: ________, 1 = she:_______. brother, 1 = sister. barbershop, 1 = salon. carpentry, 1 = sewing. bartender, 1 = hostess. physician, 1 = registered_nurse. professor, 1 = associate professor\nGoogle w2v embedding trained from the news", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0058", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0060"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0060", "chunk_index": 60, "page_numbers": [63], "text": "Typical Caucasian names, 1 = Typical African American names. Molly, 1 = Aisha. Amy, 1 = Keisha. Jake, 1 = Leroy. Luke, 1 = Jermaine. singer, 1 = rapper. lobster, 1 = shrimp. geeks, 1 = dudes. hockey, 1 = basketball. urban, 1 = slums", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0059", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0061"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0061", "chunk_index": 61, "page_numbers": [64], "text": "CONVAI\nmidwife:doctor sewing:carpentry\npediatrician:orthopedic _surgeon registered_nurse:physician\n1\n2\n0\n1\n10\n9\n9\n9\n19% of the top 150 analogies were rated as gender stereotypical by a majority of crowdworkers", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0060", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0062"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0062", "chunk_index": 62, "page_numbers": [65], "text": "- -Convolutional Neural Networks\n- -Recurrent Neural Networks", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0061", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0063"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0063", "chunk_index": 63, "page_numbers": [65], "text": "- -Long-Short Term Memory (LSTM)\n- -Gated Recurrent Units (GRU)\n- -Example Sequence Classification Tasks\n- -Elmo and Contextual Embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0062", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0064"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0064", "chunk_index": 64, "page_numbers": [66], "text": "- -Previous lectures are not recorded, but previous slides are under Modules in Canvas\n- -The homework timeline is on the main Canvas page for the course\n- -Immediate thing to act on:\n- -Form a final project team!", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0063", "next_chunk_id": null}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0000", "chunk_index": 0, "page_numbers": [1], "text": "I\nUNIVERSITY OF\nILLINOIS\nURBANA-CHAMPAIGN\nCONVAI\nCDS", "prev_chunk_id": null, "next_chunk_id": "ALecture5_SequenceModeling__0001"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0001", "chunk_index": 1, "page_numbers": [1], "text": "Dilek HakkaniT√ºr\nSieb\nCom and I", "prev_chunk_id": "ALecture5_SequenceModeling__0000", "next_chunk_id": "ALecture5_SequenceModeling__0002"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0002", "chunk_index": 2, "page_numbers": [2], "text": "- -Convolutional Neural Networks\n- -Recurrent Neural Networks\n- -Example Applications", "prev_chunk_id": "ALecture5_SequenceModeling__0001", "next_chunk_id": "ALecture5_SequenceModeling__0003"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0003", "chunk_index": 3, "page_numbers": [3, 4], "text": "- -We have discussed models that deal with paired data: input words or utterances and output categories.\n- -Example: One-hot representations or word embeddings to represent input words\n- -Sometimes data exhibits rich structure, such as images, natural language.\n- -Structure-less networks, i.e., MLPs, can fall short.\n- -CNNs: a type of NNs well-suited to detecting spatial substructure.\nme patterns are much smaller than the whole Image", "prev_chunk_id": "ALecture5_SequenceModeling__0002", "next_chunk_id": "ALecture5_SequenceModeling__0004"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0004", "chunk_index": 4, "page_numbers": [4, 5], "text": "CONVAI\ndiscover the pattern.\nConnecting to small region with fewer parameters\n- -Some patterns are much smaller than the whole image\nA neuron does not have to see the whole image to discover the pattern. +\nConnecting to small region with fewer parameters\nCONVAI\n+-\n- -The same pattern can appear in different regions.\nparameters.", "prev_chunk_id": "ALecture5_SequenceModeling__0003", "next_chunk_id": "ALecture5_SequenceModeling__0005"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0005", "chunk_index": 5, "page_numbers": [6], "text": "- -Recognizing original nationalities from last names O' Neill O' Shaughnessy Anton opoulos Kost opoulos Giann opoulos\n- -Sentiment classification A delicious breakfast was served to us at Pillerago that morning.\nThe brunch at Margoli, especially the scones, were delicious .", "prev_chunk_id": "ALecture5_SequenceModeling__0004", "next_chunk_id": "ALecture5_SequenceModeling__0006"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0006", "chunk_index": 6, "page_numbers": [7], "text": "CONVAI\n0.", "prev_chunk_id": "ALecture5_SequenceModeling__0005", "next_chunk_id": "ALecture5_SequenceModeling__0007"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0007", "chunk_index": 7, "page_numbers": [8], "text": "CONVAI", "prev_chunk_id": "ALecture5_SequenceModeling__0006", "next_chunk_id": "ALecture5_SequenceModeling__0008"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0008", "chunk_index": 8, "page_numbers": [13], "text": "CONVAI\n- -CNNs: a type of NNs well-suited to detecting spatial substructure.\nand a convolutional kernel", "prev_chunk_id": "ALecture5_SequenceModeling__0007", "next_chunk_id": "ALecture5_SequenceModeling__0009"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0009", "chunk_index": 9, "page_numbers": [14], "text": "CONVAI\n- -Dimension of the Convolution Operation", "prev_chunk_id": "ALecture5_SequenceModeling__0008", "next_chunk_id": "ALecture5_SequenceModeling__0010"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0010", "chunk_index": 10, "page_numbers": [15, 16, 17], "text": "CONVAI\n- -Channels\nThe kernel being applied to the\n- -input_channels=2, output_channels=1, kernel_size=2, stride=1, padding=0\nCONVAI\n- Channels\n- input_channels=1, output_channels=2, kernel_size=2, stride=1, padding=0\nInput tensor with 1 channel\nand two convolutional kernels\nThe kernels being applied to\nthe input tensor\nThe output tensor\nCONVAI", "prev_chunk_id": "ALecture5_SequenceModeling__0009", "next_chunk_id": "ALecture5_SequenceModeling__0011"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0011", "chunk_index": 11, "page_numbers": [17], "text": "The output matrix", "prev_chunk_id": "ALecture5_SequenceModeling__0010", "next_chunk_id": "ALecture5_SequenceModeling__0012"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0012", "chunk_index": 12, "page_numbers": [18], "text": "Stride = 1\nStride = 2", "prev_chunk_id": "ALecture5_SequenceModeling__0011", "next_chunk_id": "ALecture5_SequenceModeling__0013"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0013", "chunk_index": 13, "page_numbers": [19], "text": "CONVAI", "prev_chunk_id": "ALecture5_SequenceModeling__0012", "next_chunk_id": "ALecture5_SequenceModeling__0014"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0014", "chunk_index": 14, "page_numbers": [20], "text": "0", "prev_chunk_id": "ALecture5_SequenceModeling__0013", "next_chunk_id": "ALecture5_SequenceModeling__0015"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0015", "chunk_index": 15, "page_numbers": [20, 21], "text": "2\nCONVAI\n2\n0\n2\n0\n2\n1\n1\n0\n1\n2\n1\nBias b0 (1x1x1)\n60(8,:, 0)\nhtto://cs231n.aithub.io/convolutional-networks/\nwl[1,1,1]\n0\n2\n0[1, 1,1]\n-3\nS\n10 -3\nhttp://cs231n.github.io/convolutional-networks/\n2\n0\n2\n0\n0\n0\nD\n0", "prev_chunk_id": "ALecture5_SequenceModeling__0014", "next_chunk_id": "ALecture5_SequenceModeling__0016"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0016", "chunk_index": 16, "page_numbers": [21, 22, 23], "text": "0\nCONVAI\n2\n1\n2\n0\n-\n2\n0\n0\n0\n0\n:, 1\n0\n0\n2\n0\n0\n2\n0\nD-\n0\n2\n0\n0\n1\n1\n2\n2\n2\n1\n-1\nwl(:,:,1]\n-1\n1\n-1 0\nS\n-2\n20\no[:,:,1]\n3\n10 -3\nN\n2\n2\n0\n1\n-1\n1\n0\n0\nWO[L\n-1\n8\n-1|1\nBias b0(1x1x1)\nb9D1,01\nhttp://cs231n.github.io/convolutional-networks/\n-1\n21\n0\n0\nCONVAI\n0\n2\n1\n0\n2\nX[:,\n0\n0\n0\nX[ :\n0\n0\n2\n2\n0\n0\n0\n1\n0\n2\n0\n-1\nwl[=, 1,1]\n-1\n1\n-1\n2\n0 $\n0[1, 1,11\n5 10 -3\n0\n0\n1\n2\n1\n2\n2\n0\n2\n0\n2\n2\n2\n2\n2\n0\nWO\n0\n2\nwO/B.2d\n-1\nBias bO(1xlxl)\nbO[r,1,01\nhttp://cs231n.github.io/convolutional-networks/\nX[I\nCONVAI\n2\n0\n0\n0\n0\n2\n1\n1\n0\n0\n2\n1\n1\n2\n0\n2\n1\n1\n0\n0\n0\n2\n, 2\n0\n0\n2\n2\n2\n0\n1\n1\n2\n2\n2\n2\n0\n2\n2\n2\n2\n0\n-3\n10 -3\n:,11\n0\n2\n2\n1\n2\n2\n0\n0\n0\n0\n-1\n-1\nw0[:,:, 1]\n1\n1\n1\n0\n-1\n1+1\n-1\n1\nBias 60 (1x1x1).\nBOL: 3,01\nhttp://cs231n.github.io/convolutional-networks/\nw1[:", "prev_chunk_id": "ALecture5_SequenceModeling__0015", "next_chunk_id": "ALecture5_SequenceModeling__0017"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0017", "chunk_index": 17, "page_numbers": [24], "text": "CONVAI\n- The convolution between two functions, say ùëì , ùëî : ‚Ñù ùëë ‚Üí ùëÖ is defined as:\n- Whenever we have discrete objects, the integral turns into a sum.", "prev_chunk_id": "ALecture5_SequenceModeling__0016", "next_chunk_id": "ALecture5_SequenceModeling__0018"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0018", "chunk_index": 18, "page_numbers": [29], "text": "- -Fully-connected layer and softmax layer\n- -need fixed-size input", "prev_chunk_id": "ALecture5_SequenceModeling__0017", "next_chunk_id": "ALecture5_SequenceModeling__0019"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0019", "chunk_index": 19, "page_numbers": [30], "text": "- -choose the k-max values\n- -preserve the order of input values\n- -variable-size input, fixed-size output", "prev_chunk_id": "ALecture5_SequenceModeling__0018", "next_chunk_id": "ALecture5_SequenceModeling__0020"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0020", "chunk_index": 20, "page_numbers": [32], "text": "- -Convolutional Neural Networks\n- -Recurrent Neural Networks\n- -Example Applications", "prev_chunk_id": "ALecture5_SequenceModeling__0019", "next_chunk_id": "ALecture5_SequenceModeling__0021"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0021", "chunk_index": 21, "page_numbers": [33], "text": "- -Idea: condition the neural network on all previous words and tie the weights at each time step\n- -Assumption: temporal information matters", "prev_chunk_id": "ALecture5_SequenceModeling__0020", "next_chunk_id": "ALecture5_SequenceModeling__0022"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0022", "chunk_index": 22, "page_numbers": [34], "text": "CONVAI\nOt = softmax(V st)\nV\nU\nW\n<!-- formula-not-decoded -->\no (‚Ä¢): tanh, ReLU", "prev_chunk_id": "ALecture5_SequenceModeling__0021", "next_chunk_id": "ALecture5_SequenceModeling__0023"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0023", "chunk_index": 23, "page_numbers": [35], "text": "CONVAI\nhidden\nIdea: pass the information from the previous hidden layer to leverage all contexts", "prev_chunk_id": "ALecture5_SequenceModeling__0022", "next_chunk_id": "ALecture5_SequenceModeling__0024"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0024", "chunk_index": 24, "page_numbers": [36], "text": "CONVAI\nW", "prev_chunk_id": "ALecture5_SequenceModeling__0023", "next_chunk_id": "ALecture5_SequenceModeling__0025"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0025", "chunk_index": 25, "page_numbers": [37], "text": "CONVAI\nS\nU\nt-1\nC(0')\nt+1\npredic", "prev_chunk_id": "ALecture5_SequenceModeling__0024", "next_chunk_id": "ALecture5_SequenceModeling__0026"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0026", "chunk_index": 26, "page_numbers": [38], "text": "CONVAI", "prev_chunk_id": "ALecture5_SequenceModeling__0025", "next_chunk_id": "ALecture5_SequenceModeling__0027"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0027", "chunk_index": 27, "page_numbers": [39], "text": "CONVAI", "prev_chunk_id": "ALecture5_SequenceModeling__0026", "next_chunk_id": "ALecture5_SequenceModeling__0028"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0028", "chunk_index": 28, "page_numbers": [40], "text": "W\nCONVAI\nUnfold ut: Of\net: Y+\nC(0)\nVC(Y)\nTu x,\nx,-1\nX1+1\nX+-1\nSt-1", "prev_chunk_id": "ALecture5_SequenceModeling__0027", "next_chunk_id": "ALecture5_SequenceModeling__0029"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0029", "chunk_index": 29, "page_numbers": [41], "text": "W\nCONVAI\nUnfold\nX-1", "prev_chunk_id": "ALecture5_SequenceModeling__0028", "next_chunk_id": "ALecture5_SequenceModeling__0030"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0030", "chunk_index": 30, "page_numbers": [41], "text": "ut: 0+\net: Y+\nx,\n‚Üí O\nX,+1\nX+-1\nS+-1\nSt\nt\nYt\nC(0)\nVC(Y)", "prev_chunk_id": "ALecture5_SequenceModeling__0029", "next_chunk_id": "ALecture5_SequenceModeling__0031"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0031", "chunk_index": 31, "page_numbers": [42, 43], "text": "W\nCONVAI\nUnfold out: 0+\neT: Y+\nX-1\nx,\nX,+1\nX+-1\n- Input: init, x\n1 , x 2 , ‚Ä¶, x t\n- Output: o\nt\n- Target: y\nt\nX+\nSt-1\nC(0)\nVC(Y)\nW\nSt\nCONVAI\nW\n‚Ä¢\nUnfold\nW\nx-1", "prev_chunk_id": "ALecture5_SequenceModeling__0030", "next_chunk_id": "ALecture5_SequenceModeling__0032"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0032", "chunk_index": 32, "page_numbers": [43], "text": "ut: Of et: Y+\nTu\n*,\n‚Üí –û\nX,+1\nt-\nX+\nS+-1\nYt\nC(0)\nVC(y)\nW\nW", "prev_chunk_id": "ALecture5_SequenceModeling__0031", "next_chunk_id": "ALecture5_SequenceModeling__0033"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0033", "chunk_index": 33, "page_numbers": [44], "text": "CONVAI\nW\n‚Ä¢\nUnfold\nX,-1", "prev_chunk_id": "ALecture5_SequenceModeling__0032", "next_chunk_id": "ALecture5_SequenceModeling__0034"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0034", "chunk_index": 34, "page_numbers": [44], "text": "ut: Of et: Y+\n*,\n‚Üí –û\nt+/\nX,+1\nX+- yt\nC(0)\nVC(V)\nW\n- Input: init, x 1 , x 2 , ‚Ä¶, x t\n- Output: o t\n- Target: y\nt", "prev_chunk_id": "ALecture5_SequenceModeling__0033", "next_chunk_id": "ALecture5_SequenceModeling__0035"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0035", "chunk_index": 35, "page_numbers": [45], "text": "CONVAI\nBackward Pass:\nS2\nS3\nSA", "prev_chunk_id": "ALecture5_SequenceModeling__0034", "next_chunk_id": "ALecture5_SequenceModeling__0036"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0036", "chunk_index": 36, "page_numbers": [46], "text": "CONVAI\n- -The gradient is a product of Jacobian matrices, each associated with a step in the forward computation\n- -Multiply the same matrix at each time step during backprop\nThe gradient becomes very small or very large quickly ‚Üí vanishing or exploding gradient", "prev_chunk_id": "ALecture5_SequenceModeling__0035", "next_chunk_id": "ALecture5_SequenceModeling__0037"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0037", "chunk_index": 37, "page_numbers": [47, 48], "text": "CONVAI\n¬ß + threshold g if ||g|l ‚â• threshold then\nend if\nIdea: control the gradient value to avoid exploding\nParameter setting: values from half to ten times the average can still yield convergence\nho)", "prev_chunk_id": "ALecture5_SequenceModeling__0036", "next_chunk_id": "ALecture5_SequenceModeling__0038"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0038", "chunk_index": 38, "page_numbers": [48], "text": "- -RNN models temporal sequence information\n- -can handle 'long -term dependencies' in theory\nIssue: RNN cannot handle such 'long -term dependencies' i n practice due to vanishing gradient ‚Üí apply the gating mechanism to directly encode the long-distance information", "prev_chunk_id": "ALecture5_SequenceModeling__0037", "next_chunk_id": "ALecture5_SequenceModeling__0039"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0039", "chunk_index": 39, "page_numbers": [49], "text": "CONVAI\nh. = f(Wx, + Vhis +b)\ny, = 8(Ulhi;h,l+c)\n‚Ñé = ‚Ñé; ‚Ñé represents (summarizes) the past and future around a single token", "prev_chunk_id": "ALecture5_SequenceModeling__0038", "next_chunk_id": "ALecture5_SequenceModeling__0040"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0040", "chunk_index": 40, "page_numbers": [50], "text": "CONVAI\nh(3)\nh(2)\nh(\")\n‚Üí(i)‚Üí(i)\nh=fWni- + h-+–ë\")\n+(i)\n-(i)\nhi= f(W\"), h;-\" + F\")-(i)\nhitl+b )\nEach memory layer passes an intermediate representation to the next", "prev_chunk_id": "ALecture5_SequenceModeling__0039", "next_chunk_id": "ALecture5_SequenceModeling__0041"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0041", "chunk_index": 41, "page_numbers": [51], "text": "- -Convolutional Neural Networks\n- -Recurrent Neural Networks\n- -Example Applications", "prev_chunk_id": "ALecture5_SequenceModeling__0040", "next_chunk_id": "ALecture5_SequenceModeling__0042"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0042", "chunk_index": 42, "page_numbers": [52], "text": "CONVAI\nlike this\nmovie very\nmuch\n!\n0.8\n0.9 0.1\n0.5\n0.1\n0.4 | 0.8| 0.1 |-0.1| 0.7\n- -Example depiction from: http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnnperform-text-classification-with-word-embeddings/\n0.51\n0.1\n0.1|\n0.4\n0.1\n0.1", "prev_chunk_id": "ALecture5_SequenceModeling__0041", "next_chunk_id": "ALecture5_SequenceModeling__0043"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0043", "chunk_index": 43, "page_numbers": [53, 54], "text": "CONVAI\nlike this\nmovie very\nmuch\n!\n- -Example depiction from: http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnnperform-text-classification-with-word-embeddings/\n0.8\n0.5 0.2\n0.8 | 0.9 0.1 0.5\n0.4\nDialogue Act classhlication", "prev_chunk_id": "ALecture5_SequenceModeling__0042", "next_chunk_id": "ALecture5_SequenceModeling__0044"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0044", "chunk_index": 44, "page_numbers": [54], "text": "CONVAI\nPooling\n- -(Lee & Dernoncourt, NAACL, 2016)\n- -Dialogue Act Classification\nXe\nRNN\n1\nX1\nh2\nRNN\n1\nX2\nPooling h3\nRNN\nX3\n***.\nhe\nRNN\nxe", "prev_chunk_id": "ALecture5_SequenceModeling__0043", "next_chunk_id": "ALecture5_SequenceModeling__0045"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0045", "chunk_index": 45, "page_numbers": [55], "text": "CONVAI\nLSTMO\nLSTM,\nWo\nWo\nLSTM, h1\nWt\n.. .\n- -(Ravuri and Stolcke, Interspeech, 2015)\n<S>\nW1\n<5>\n- -Addressee Detection\n<5>\nW1\n</s>\nWo\nPv", "prev_chunk_id": "ALecture5_SequenceModeling__0044", "next_chunk_id": "ALecture5_SequenceModeling__0046"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0046", "chunk_index": 46, "page_numbers": [56], "text": "CONVAI\nSlot tagging\n- -(Mesnil et al., IEEE TASLP, 2015)\n- -Slot tagging\nho\nh\nWo\nWz\n(a) RNN\nWn\n72\nYo\nXhi\nW1\nWo\n(b) RNN-LA\n–£ 2\n72\nh\nThi\nWo Wi\n(c) bi-RNN\nB-contact_name\n< END>\nB-subject I-subject I-subject send_email\nC\nTayl", "prev_chunk_id": "ALecture5_SequenceModeling__0045", "next_chunk_id": "ALecture5_SequenceModeling__0047"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0047", "chunk_index": 47, "page_numbers": [57], "text": "- -(HakkaniT√ºr et al., Interspeech, 2016 )\n- -Slot filling (or tagging) and intent prediction in the same output sequence", "prev_chunk_id": "ALecture5_SequenceModeling__0046", "next_chunk_id": "ALecture5_SequenceModeling__0048"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0048", "chunk_index": 48, "page_numbers": [58], "text": "CONVAI\n- -(Tafforeau et al., Interspeech, 2016)\nbackward\n- -Goal: exploit data from domains/tasks with a lot of data to improve ones with less data\n- -Lower layers are shared across domains/tasks\n- -Output layer is specific to task", "prev_chunk_id": "ALecture5_SequenceModeling__0047", "next_chunk_id": "ALecture5_SequenceModeling__0049"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0049", "chunk_index": 49, "page_numbers": [59], "text": "- -Long-Short Term Memory (LSTM)\n- -Gated Recurrent Units (GRU)\n- -Example Sequence Classification Tasks\n- -Elmo and Contextual Embeddings", "prev_chunk_id": "ALecture5_SequenceModeling__0048", "next_chunk_id": "ALecture5_SequenceModeling__0050"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0050", "chunk_index": 50, "page_numbers": [60], "text": "- -September 30 th\n- -In class\n- -True/False and multiple-choice questions from content we discussed in class.", "prev_chunk_id": "ALecture5_SequenceModeling__0049", "next_chunk_id": "ALecture5_SequenceModeling__0051"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0051", "chunk_index": 51, "page_numbers": [61], "text": "- -Final Project Proposal team sign up deadline: Sept 23 rd\n- -Spreadsheet to sign up project teams: https://docs.google.com/spreadsheets/d/1EJ_5Xby0mRhHFmSRSmxl v6Gws4Qs5T8P5JUiKYKAZcA/edit?usp=sharing\n- -Reach out to me or TAs soon if you need help with project ideas and teaming.", "prev_chunk_id": "ALecture5_SequenceModeling__0050", "next_chunk_id": null}
