## this is to test llm.py##

python - <<'PY'
import os, json, asyncio
from pathlib import Path
from collections import defaultdict

from dotenv import load_dotenv

# Load .env from current directory
env_path = Path.cwd() / ".env"
load_dotenv(dotenv_path=env_path, override=True)

from llm import extract_concepts_with_roles_from_chunks, build_concept_cards

def _require_file(path: Path) -> None:
    if not path.exists():
        raise SystemExit(f"Missing file: {path} (CWD={Path.cwd()})")

def chunk_concepts_from_mentions(mentions):
    m = defaultdict(list)
    for r in mentions:
        m[r["chunk_id"]].append(r["concept_id"])
    return {cid: sorted(set(v)) for cid, v in m.items()}

async def main():
    print("CWD =", Path.cwd())
    print("ENV  =", env_path, "(exists)" if env_path.exists() else "(missing)")
    print("LLM_PROVIDER =", os.getenv("LLM_PROVIDER"))
    print("LLM_MODEL    =", os.getenv("LLM_MODEL"))
    print("OPENAI_CONCEPTS_MODEL =", os.getenv("OPENAI_CONCEPTS_MODEL"))

    chunks_path = Path("out/chunks.jsonl")
    _require_file(chunks_path)

    chunks = []
    with chunks_path.open("r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            chunks.append(json.loads(line))
            if len(chunks) >= 2:   # keep small so it never “hangs”
                break

    print(f"Loaded {len(chunks)} chunk(s) from {chunks_path}")

    print("Running extraction (LLM) on 4 chunks...")
    mentions = await extract_concepts_with_roles_from_chunks(chunks)

    print("mentions len =", len(mentions))
    print("First mentions:")
    print(json.dumps(mentions[:5], indent=2, ensure_ascii=False))

    print("\nBuilding ConceptCards (offline)...")
    chunk_concepts = chunk_concepts_from_mentions(mentions)
    cards = build_concept_cards(mentions, chunk_concepts)

    print("cards len =", len(cards))
    print("First cards:")
    print(json.dumps(cards[:3], indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(main())
PY

"": "meta-llama/Llama-3.2-3B-Instruct"


## this is to test llm.py##

python - <<'PY'
import os, json, asyncio
from pathlib import Path
from collections import defaultdict
from dotenv import load_dotenv

# Load .env first (so we can override it below)
env_path = Path.cwd() / ".env"
load_dotenv(dotenv_path=env_path, override=True)

# Force ONE provider + ONE model for this test run
os.environ["LLM_PROVIDER"] = "hf"
os.environ["LLM_MODEL"] = "llama3b"  # maps to meta-llama/Llama-3.2-3B-Instruct in adapters.py

# ✅ FORCE CPU for THIS run (prevents MPS crash)
os.environ["HF_FORCE_CPU"] = "1"

from adapters import _resolve_model, HF_MODELS_MAP
from llm import extract_concepts_with_roles_from_chunks, build_concept_cards

def _require_file(path: Path) -> None:
    if not path.exists():
        raise SystemExit(f"Missing file: {path} (CWD={Path.cwd()})")

def chunk_concepts_from_mentions(mentions):
    m = defaultdict(list)
    for r in mentions:
        m[r["chunk_id"]].append(r["concept_id"])
    return {cid: sorted(set(v)) for cid, v in m.items()}

async def main():
    provider = os.getenv("LLM_PROVIDER")
    requested = os.getenv("LLM_MODEL")

    resolved = _resolve_model(provider, requested)
    hf_model_id = HF_MODELS_MAP.get(resolved, resolved)

    print("CWD =", Path.cwd())
    print("ENV  =", env_path, "(exists)" if env_path.exists() else "(missing)")
    print("LLM_PROVIDER =", provider)
    print("LLM_MODEL    =", requested)
    print("HF_FORCE_CPU =", os.getenv("HF_FORCE_CPU"))
    print("Resolved     =", resolved)
    print("HF model id  =", hf_model_id)

    chunks_path = Path("out/chunks.jsonl")
    _require_file(chunks_path)

    chunks = []
    with chunks_path.open("r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            chunks.append(json.loads(line))
            if len(chunks) >= 2:
                break

    print(f"Loaded {len(chunks)} chunk(s) from {chunks_path}")
    print("Running extraction (LLM) on 4 chunks...")
    mentions = await extract_concepts_with_roles_from_chunks(chunks)

    print("mentions len =", len(mentions))
    print("First mentions:")
    print(json.dumps(mentions[:5], indent=2, ensure_ascii=False))

    print("\nBuilding ConceptCards (offline)...")
    chunk_concepts = chunk_concepts_from_mentions(mentions)
    cards = build_concept_cards(mentions, chunk_concepts)

    print("cards len =", len(cards))
    print("First cards:")
    print(json.dumps(cards[:3], indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(main())
PY
